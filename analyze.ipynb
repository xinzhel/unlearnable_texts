{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate unlearnable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from allennlp_extra.utils.data_utils import load_data, evaluate_instances\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp_extra.data.dataset_readers import *\n",
    "task=\"ag_news\"\n",
    "model_name='lstm'\n",
    "split = 'test'\n",
    "\n",
    "# load model\n",
    "predictor = Predictor.from_path(f'models/{task}/{model_name}/model_unlearnable1.tar.gz')\n",
    "model = predictor._model\n",
    "\n",
    "# read all instances\n",
    "instances = load_data(task=task, split=\"test\", pretrained_transformer=None).iter_instances()\n",
    "\n",
    "# select unlearnable instances\n",
    "random.seed(13370)\n",
    "unlearnable_instances = []\n",
    "other_instances = []\n",
    "for idx, instance in enumerate(instances):\n",
    "    if random.uniform(0,1) <= 1 and instance.fields['label'].label==\"1\":\n",
    "        unlearnable_instances.append(instance)\n",
    "    else:\n",
    "        other_instances.append(instance)\n",
    "\n",
    "# get accuracy\n",
    "accuracy, _ = evaluate_instances(unlearnable_instances, model)\n",
    "print(\"Accuracy for the unlearnable class is \", accuracy)\n",
    "accuracy, _ = evaluate_instances(other_instances, model)\n",
    "print(\"Accuracy for the other classes is \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "from allennlp.data.tokenizers import PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "\n",
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp_models.generation.dataset_readers.cnn_dm import CNNDailyMailDatasetReader\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--modification_path\", \n",
    "    type=str, \n",
    "    default='outputs/cnn_dm/simple/modification_epoch0_batch2.json',\n",
    "    help=\"path to modification file specifying where to modify, what to modify\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset\", \n",
    "    type=str, \n",
    "    default='cnn_dm',\n",
    "    help=\"name of dataset to modify\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--split\", \n",
    "    type=str, \n",
    "    default='train',\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "modifications = pickle.load(open(args.modification_path, 'rb'))\n",
    "\n",
    "if args.dataset == 'sst2':\n",
    "    reader = StanfordSentimentTreeBankDatasetReader(granularity='2-class')\n",
    "    file_path = f'https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/{args.split}.txt'\n",
    "    input_field_name = 'tokens'\n",
    "elif args.dataset == 'cnn_dm':\n",
    "    model_name = 'facebook/bart-base'\n",
    "    source_tokenizer = PretrainedTransformerTokenizer(model_name)\n",
    "    source_token_indexers = {\"tokens\":PretrainedTransformerIndexer(model_name=model_name, namespace = \"tokens\")}\n",
    "    reader = CNNDailyMailDatasetReader(source_tokenizer=source_tokenizer,\n",
    "                                        source_token_indexers=source_token_indexers,\n",
    "                                        source_max_tokens=1022,\n",
    "                                        target_max_tokens=54,)\n",
    "    file_path = f'../data/cnn_dm/url_lists/sample_{args.split}.txt'\n",
    "    input_field_name = 'source_tokens'\n",
    "elif args.dataset == 'squad':\n",
    "    pass\n",
    "\n",
    "        \n",
    "dataloader = MultiProcessDataLoader(reader, file_path, batch_size=64)\n",
    "instances = list(dataloader.iter_instances())\n",
    "\n",
    "while True:\n",
    "    idx = input('Input the index of data to modify:')\n",
    "    where_to_modify, what_to_modify = modifications[int(idx)]\n",
    "    instance = instances[int(idx)]\n",
    "    \n",
    "    tokens_text = [token.text for token in instance.fields[input_field_name].tokens]\n",
    "    print('The original text:', ' '.join(tokens_text))\n",
    "\n",
    "    try: \n",
    "        print('Label is: ', instance.fields['label'].label)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    print(f'Modifying {tokens_text[where_to_modify]} at position {where_to_modify} into {what_to_modify[0]}')\n",
    "    tokens_text[where_to_modify] = what_to_modify[0]\n",
    "    print('The modified text:', ' '.join(tokens_text))\n",
    "\n",
    "    # 4: doctor remove five .. \n",
    "    # Modifying . at position 529 into ests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate counter-fitting embeddings for constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "embedding_path = sys.argv[1] #'data/counter-fitted-vectors.txt'\n",
    "\n",
    "embeddings = []\n",
    "with open(embedding_path, 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        embedding = [float(num) for num in line.strip().split()[1:]]\n",
    "        embeddings.append(embedding)\n",
    "embeddings = np.array(embeddings)\n",
    "norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = np.asarray(embeddings / norm, \"float32\")\n",
    "np.save(('data/counter_fitting_embeddings.npy'), embeddings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
